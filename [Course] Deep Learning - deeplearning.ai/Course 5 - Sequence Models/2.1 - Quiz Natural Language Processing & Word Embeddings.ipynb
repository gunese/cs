{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 - Quiz: Natural Language Processing & Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words.\n",
    "\n",
    "**False**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "What is t-SNE?\n",
    "\n",
    "**A non-linear dimensionality reduction technique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "Suppose you download a pre-trained word embedding which has been trained on a huge corpus of text. You then use this word embedding to train an RNN for a language task of recognizing if someone is happy from a short snippet of text, using a small training set.\n",
    "\n",
    "| x (input text)               \t| y (happy?) \t|\n",
    "|------------------------------\t|------------\t|\n",
    "| I'm feeling wonderful today! \t| 1          \t|\n",
    "| I'm bummed my cat is ill.    \t| 0          \t|\n",
    "| Really enjoying this!        \t| 1          \t|\n",
    "\n",
    "Then even if the word “ecstatic” does not appear in your small training set, your RNN might reasonably be expected to recognize “I’m ecstatic” as deserving a label $y = 1$.\n",
    "\n",
    "**True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "Which of these equations do you think should hold for a good word embedding? (Check all that apply)\n",
    "\n",
    "**$e_{boy} − e_{girl} \\approx e_{brother} − e_{sister}$**\n",
    "\n",
    "**$e_{boy} − e_{brother} \\approx e_{girl} − e_{sister}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "Let $E$ be an embedding matrix, and let $o_{1234}$ be a one-hot vector corresponding to word 1234. Then to get the embedding of word 1234, why don’t we call $E * o_{1234}$ in Python?\n",
    "\n",
    "**It is computationally wasteful.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**6.**\n",
    "When learning word embeddings, we create an artificial task of estimating $P(target \\mid context)$. It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.\n",
    "\n",
    "**True**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**7.**\n",
    "In the word2vec algorithm, you estimate $P(t \\mid c)$, where $t$ is the target word and $c$ is a context word. How are $t$ and $c$ chosen from the training set? Pick the best answer.\n",
    "\n",
    "**$c$ and $t$ are chosen to be nearby words.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**8.**\n",
    "Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings. The word2vec model uses the following softmax function:\n",
    "\n",
    "$P(t \\mid c) = \\frac{e{\\theta}^T_t e c}{\\sum^{10000}_t e{\\theta}^T_t e c}$\n",
    "\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "**$\\theta_t$ and $e_c$ are both 500 dimensional vectors.**\n",
    "\n",
    "**$θ_t$ and $e_c$ are both trained with an optimization algorithm such as Adam or gradient descent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**9.**\n",
    "Suppose you have a 10000 word vocabulary, and are learning 500-dimensional word embeddings.The GloVe model minimizes this objective:\n",
    "\n",
    "$min∑^{10,000}_{i=1} ∑^{10,000}_{j=1} f(X_ij)(θ^T_i e_j + b_i + b^′_j − log X_ij)^2$\n",
    "\n",
    "Which of these statements are correct? Check all that apply.\n",
    "\n",
    "**$θ_i$ and $e_j$ should be initialized randomly at the beginning of training.**\n",
    "\n",
    "**$X_{ij}$ is the number of times word i appears in the context of word j.**\n",
    "\n",
    "**The weighting function $f(.)$ must satisfy $f(0) = 0$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**10.**\n",
    "You have trained word embeddings using a text dataset of $m_1$ words. You are considering using these word embeddings for a language task, for which you have a separate labeled dataset of $m_2$ words. Keeping in mind that using word embeddings is a form of transfer learning, under which of these circumstance would you expect the word embeddings to be helpful?\n",
    "\n",
    "**$m_1 >> m_2$**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
