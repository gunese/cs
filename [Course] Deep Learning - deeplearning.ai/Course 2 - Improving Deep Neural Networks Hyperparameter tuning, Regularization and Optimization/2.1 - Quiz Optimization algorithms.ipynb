{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2 - Quiz: Optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "Which notation would you use to denote the 3rd layer’s activations when the input is the 7th example from the 8th minibatch?\n",
    "\n",
    "**$a^{[3]\\{8\\}(7)}$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "Which of these statements about mini-batch gradient descent do you agree with?\n",
    "\n",
    "**One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "Why is the best mini-batch size usually not 1 and not m, but instead something in-between?\n",
    "\n",
    "**If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch.**\n",
    "\n",
    "**If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "Suppose your learning algorithm’s cost $J$, plotted as a function of the number of iterations, looks like this:\n",
    "\n",
    "![picture alt](https://i.ibb.co/whc0843/KIycr3gr-Eee-JIwr-F5-BVs-Ig-f1c324824bd9220c7ee985cce1521404-cost.png)\n",
    "\n",
    "Which of the following do you agree with?\n",
    "\n",
    "**If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "Suppose the temperature in Casablanca over the first three days of January are the same:\n",
    "\n",
    "Jan 1st: $\\theta_1 = 10^o C$\n",
    "\n",
    "Jan 2nd: $\\theta_2 10^o C$\n",
    "\n",
    "(We used Fahrenheit in lecture, so will use Celsius here in honor of the metric world.)\n",
    "\n",
    "Say you use an exponentially weighted average with $\\beta = 0.5$ to track the temperature: $v_0 = 0, v_t = \\beta v_{t-1} +(1-\\beta)\\theta_t$ is the value computed after day 2 without bias correction, and $v_2^{corrected}$ is the value you compute with bias correction. What are these values? (You might be able to do this without a calculator, but you don't actually need one. Remember what is bias correction doing.)\n",
    "\n",
    "**$v_2=7.5, v_2^{corrected} = 10$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**6.**\n",
    "Which of these is NOT a good learning rate decay scheme? Here, t is the epoch number.\n",
    "\n",
    "**$\\alpha = e^t \\alpha_0$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**7.**\n",
    "You use an exponentially weighted average on the London temperature dataset. You use the following to track the temperature: $v_{t} = \\beta v_{t-1} + (1-\\beta)\\theta_t$. The red line below was computed using $\\beta = 0.9$. What would happen to your red curve as you vary $\\beta$? (Check the two that apply)\n",
    "\n",
    "![picture alt](https://i.ibb.co/FJ5b4jc/KIycr3gr-Eee-JIwr-F5-BVs-Ig-f1c324824bd9220c7ee985cce1521404-cost.png)\n",
    "\n",
    "**Increasing $\\beta$ will shift the red line slightly to the right.**\n",
    "\n",
    "**Decreasing $\\beta$ will create more oscillation within the red line.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**8.**\n",
    "Consider this figure:\n",
    "\n",
    "![picture alt](https://i.ibb.co/FBztLfv/KIycr3gr-Eee-JIwr-F5-BVs-Ig-f1c324824bd9220c7ee985cce1521404-cost.png)\n",
    "\n",
    "These plots were generated with gradient descent; with gradient descent with momentum ($\\beta$ = 0.5) and gradient descent with momentum ($\\beta$ = 0.9). Which curve corresponds to which algorithm?\n",
    "\n",
    "**(1) is gradient descent. (2) is gradient descent with momentum (small $\\beta$). (3) is gradient descent with momentum (large $\\beta$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**9.**\n",
    "Suppose batch gradient descent in a deep network is taking excessively long to find a value of the parameters that achieves a small value for the cost function $\\mathcal{J}(W^{[1]},b^{[1]},..., W^{[L]},b^{[L]})$. Which of the following techniques could help find parameter values that attain a small value for $\\mathcal{J}$? (Check all that apply)\n",
    "\n",
    "**Try using Adam**\n",
    "\n",
    "**Try better random initialization for the weights**\n",
    "\n",
    "**Try mini-batch gradient descent**\n",
    "\n",
    "**Try tuning the learning rate $\\alpha$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**10.**\n",
    "Which of the following statements about Adam is False?\n",
    "\n",
    "**Adam should be used with batch gradient computations, not with mini-batches.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
