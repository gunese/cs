{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 1 - Quiz: Practical aspects of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "If you have 10,000,000 examples, how would you split the train/dev/test set?\n",
    "\n",
    "**98% train . 1% dev . 1% test**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "The dev and test set should:\n",
    "\n",
    "**Come from the same distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "If your Neural Network model seems to have high bias, what of the following would be promising things to try? (Check all that apply.)\n",
    "\n",
    "**Increase the number of units in each hidden layer**\n",
    "\n",
    "**Make the Neural Network deeper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)\n",
    "\n",
    "**Increase the regularization parameter lambda**\n",
    "\n",
    "**Get more training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "What is weight decay?\n",
    "\n",
    "**A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**6.**\n",
    "What happens when you increase the regularization hyperparameter lambda?\n",
    "\n",
    "**Weights are pushed toward becoming smaller (closer to 0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**7.**\n",
    "With the inverted dropout technique, at test time:\n",
    "\n",
    "**You do not apply dropout (do not randomly eliminate units) and do not keep the 1/keep_prob factor in the calculations used in training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**8.**\n",
    "Increasing the parameter keep_prob from (say) 0.5 to 0.6 will likely cause the following: (Check the two that apply)\n",
    "\n",
    "**Reducing the regularization effect**\n",
    "\n",
    "**Causing the neural network to end up with a lower training set error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**9.**\n",
    "Which of these techniques are useful for reducing variance (reducing overfitting)? (Check all that apply.)\n",
    "\n",
    "**Data augmentation**\n",
    "\n",
    "**L2 regularization**\n",
    "\n",
    "**Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**10.**\n",
    "Why do we normalize the inputs $x$?\n",
    "\n",
    "**It makes the cost function faster to optimize**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
