{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 5 - Practice Quiz: Lagrange multipliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "In this quiz we will consider Lagrange multipliers as a technique to find a minimum of a function subject to a constraint, i.e. solutions lying on a particular curve.\n",
    "\n",
    "Let's consider the example of finding the minimum of the function,\n",
    "\n",
    "$f(\\mathbf{x}) = \\exp\\left( -\\frac{2x^2+y^2 - xy}{2} \\right)$\n",
    "\n",
    "along the curve (or, subject to the constraint),\n",
    "\n",
    "$g(\\mathbf{x}) = x^2 + 3 (y+1)^2 -1 = 0 \\;.$\n",
    "\n",
    "The functions themselves are fairly simple, on a contour map they look as follows,\n",
    "\n",
    "![picture alt](https://i.ibb.co/y0FtQMW/bt-jjw-H3-Eeio-Q5-TVYls-AA-2a986eebdf60a9ce34283dd7650cb0d2-contour.png)\n",
    "\n",
    "However, their combination can become quite complicated if they were computed directly, as can be inferred from the shape of the constraint on the surface plot,\n",
    "\n",
    "![picture alt](https://i.ibb.co/WHHQMH2/bt-jjw-H3-Eeio-Q5-TVYls-AA-2a986eebdf60a9ce34283dd7650cb0d2-contour.png)\n",
    "\n",
    "Do note, in this case, the function $f(\\mathbf{x})$ does not have any minima itself, but along the curve, there are two minima (and two maxima).\n",
    "\n",
    "A situation like this is where Lagrange multipliers come in. The observation is that the maxima and minima on the curve, will be found where the constraint is parallel to the contours of the function.\n",
    "\n",
    "Since the gradient is perpendicular to the contours, the gradient of the function and the gradient of the constraint will also be parallel, that is,\n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = \\lambda \\nabla g(\\mathbf{x})$\n",
    "\n",
    "If we write this out in component form, this becomes,\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\frac{\\partial{f}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{f}}{\\partial{y}}\n",
    "\\end{bmatrix} = \\lambda\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{g}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{g}}{\\partial{y}}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This equation, along with $g(\\mathbf{x}) = 0$ is enough to specify the system fully. We can put all this information into a single vector equation,\n",
    "\n",
    "$\\nabla\\mathcal{L}(x, y, \\lambda) = \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial{f}}{\\partial{x}} - \\lambda \\frac{\\partial{g}}{\\partial{x}} \\\\\n",
    "\\frac{\\partial{f}}{\\partial{y}} - \\lambda \\frac{\\partial{g}}{\\partial{y}} \\\\\n",
    "-g(\\mathbf{x})\n",
    "\\end{bmatrix} = 0$\n",
    "\n",
    "Let's reflect on what we have done here, we have converted from a question of finding a minimum of a 2D function constrained to a 1D curve, to finding the zeros of a 3D vector equation.\n",
    "\n",
    "Whereas this may sound like we have made the problem more complicated, we are exactly equipped to deal with this kind of problem; we can use the root finding methods, such as the Newton-Raphson method that we've discussed previously.\n",
    "\n",
    "Let's set up the system,\n",
    "\n",
    "The function and two of the derivatives are defined for you. Set up the other two by replacing the question marks in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define the functions,\n",
    "def f (x, y) :\n",
    "    return np.exp(-(2*x*x + y*y - x*y) / 2)\n",
    "\n",
    "def g (x, y) :\n",
    "    return x*x + 3*(y+1)**2 - 1\n",
    "\n",
    "# Next their derivatives,\n",
    "def dfdx (x, y) :\n",
    "    return 1/2 * (-4*x + y) * f(x, y)\n",
    "\n",
    "def dfdy (x, y) :\n",
    "    return 1/2 * (x - 2*y) * f(x, y)\n",
    "\n",
    "def dgdx (x, y) :\n",
    "    return 2*x\n",
    "\n",
    "def dgdy (x, y) :\n",
    "    return 6*(y+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dgdx = 2*x**\n",
    "\n",
    "**dgdy = 6*(y+1)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "Next let's define the vector, $\\nabla\\mathcal{L}$, that we are to find the zeros of; we'll call this “DL” in the code. Then we can use a pre-written root finding method in scipy to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = -0.958963\n",
      "y = -1.1637\n",
      "λ = -0.246538\n",
      "f(x, y) = 0.353902\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "def DL (xyλ) :\n",
    "    [x, y, λ] = xyλ\n",
    "    return np.array([\n",
    "            dfdx(x, y) - λ * dgdx(x, y),\n",
    "            dfdy(x, y) - λ * dgdy(x, y),\n",
    "            - g(x, y)\n",
    "        ])\n",
    "\n",
    "(x0, y0, λ0) = (-1, -1, 0)\n",
    "x, y, λ = optimize.root(DL, [x0, y0, λ0]).x\n",
    "print(\"x = %g\" % x)\n",
    "print(\"y = %g\" % y)\n",
    "print(\"λ = %g\" % λ)\n",
    "print(\"f(x, y) = %g\" % f(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the first two elements of the array are the $x$ and $y$ coordinates that we wanted to find, and the last element is the Lagrange multiplier, which we can throw away now it has been used.\n",
    "\n",
    "Check that $(x, y)$ does indeed solve the equation $g(x, y) = 0$.\n",
    "\n",
    "You should be able to use the code find the other roots of the system.\n",
    "\n",
    "Re-use the code above with different starting values (on line 11) to find the other stationary points on the constraint.\n",
    "\n",
    "There are four in total. Give the $y$ coordinate of any of the other solutions to two decimal places.\n",
    "\n",
    "**-1.21**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "In the previous question, you gave the $y$ coordinate of any of the stationary points. In this part, give the $x$ coordinate of the global minimum of $f(x$ on $g(x)=0$.\n",
    "\n",
    "Give your answer to 2 decimal places.\n",
    "\n",
    "**0.93**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "You may be wondering about why the vector $\\nabla\\mathcal{L}$ gets a funny symbol. This is because it can be written as the gradient (over $x$, $y$, and $\\lambda$) of a scalar function $\\mathcal{L}(x, y, \\lambda)$.\n",
    "\n",
    "Based on your knowledge of derivatives, what function $\\mathcal{L}(x, y, \\lambda)$ would give the expected form of $\\nabla\\mathcal{L}$?\n",
    "\n",
    "**$\\mathcal{L}(x, y, \\lambda) = f(\\mathbf{x}) - \\lambda g(\\mathbf{x})$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "Hopefully you've now built up a feeling for how Lagrange multipliers work. Let's test this out on a new function and constraint.\n",
    "\n",
    "Calculate the minimum of\n",
    "\n",
    "$f(x, y) = -\\exp(x-y^2+x y)$ \n",
    "\n",
    "on the constraint,\n",
    "\n",
    "$g(x, y) = \\cosh(y) + x - 2 = 0$\n",
    "\n",
    "Use the code you've written in the previous questions to help you. You will want to change the expressions \"???\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 0.957782\n",
      "y = 0.289565\n",
      "λ = -4.07789\n",
      "f(x, y) = -3.16222\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "# First we define the functions, YOU SHOULD IMPLEMENT THESE\n",
    "def f (x, y) :\n",
    "    return -np.exp(x - y**2 + x*y)\n",
    "\n",
    "def g (x, y) :\n",
    "    return np.cosh(y) + x - 2\n",
    "\n",
    "# Next their derivatives, YOU SHOULD IMPLEMENT THESE\n",
    "def dfdx (x, y) :\n",
    "    return -(y+1)*np.exp(x-y**2+x*y)\n",
    "\n",
    "def dfdy (x, y) :\n",
    "    return (2*y-x)*np.exp(x-y**2+x*y)\n",
    "\n",
    "def dgdx (x, y) :\n",
    "    return 1\n",
    "\n",
    "def dgdy (x, y) :\n",
    "    return np.sinh(y)\n",
    "\n",
    "# Use the definition of DL from previously.\n",
    "def DL (xyλ) :\n",
    "    [x, y, λ] = xyλ\n",
    "    return np.array([\n",
    "            dfdx(x, y) - λ * dgdx(x, y),\n",
    "            dfdy(x, y) - λ * dgdy(x, y),\n",
    "            - g(x, y)\n",
    "        ])\n",
    "\n",
    "# To score on this question, the code above should set\n",
    "# the variables x, y, λ, to the values which solve the\n",
    "# Langrange multiplier problem.\n",
    "\n",
    "# I.e. use the optimize.root method, as you did previously.\n",
    "(x0, y0, λ0) = (0.5, 0.5, 1)\n",
    "x, y, λ = optimize.root(DL, [x0, y0, λ0]).x\n",
    "\n",
    "print(\"x = %g\" % x)\n",
    "print(\"y = %g\" % y)\n",
    "print(\"λ = %g\" % λ)\n",
    "print(\"f(x, y) = %g\" % f(x, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
