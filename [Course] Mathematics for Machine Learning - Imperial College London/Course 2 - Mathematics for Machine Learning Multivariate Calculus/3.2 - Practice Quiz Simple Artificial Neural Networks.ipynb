{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Practice Quiz: Simple Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "Recall from the video the structure of one of the simplest neural networks,\n",
    "\n",
    "![picture alt](https://i.ibb.co/zJ6y4nh/JKIDlfih-Eee-APQpr-C3-K2-Bg-66921c651e175da7cc8b805860da4225-simple.png)\n",
    "\n",
    "Here there are only two neurons (or nodes), and they are linked by a single edge.\n",
    "\n",
    "The _activation_ of neurons in the final layer, (1), is determined by the activation of neurons in the previous layer, (0),\n",
    "\n",
    "$a(1)= \\sigma(w^{(1)} a^{(0)} + b^{(1)})$,\n",
    "\n",
    "where $w^{(1)}$ is the weight of the connection between Neuron (0) and Neuron (1), and $b^{(1)}$ is the bias of the Neuron (1). These are then subject to the _activation function_, $\\sigma$ to give the activation of Neuron (1)\n",
    "\n",
    "Our small neural network won't be able to do a lot - it's far too simple. It is however worth plugging a few numbers into it to get a feel for the parts.\n",
    "\n",
    "Let's assume we want to train the network to give a _NOT function_, that is if you input 1 it returns 0, and if you input 0 it returns 1.\n",
    "\n",
    "For simplicity, let's use, $\\sigma(z) = \\tanh(z)$, for our activation function, and _randomly_ initialize our weight and bias to $w^{(1)}=1.3$ and $b^{(1)} = -0.1$.\n",
    "\n",
    "Use the code block below to see what output values the neural network initially returns for training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set the state of the network\n",
    "σ = np.tanh\n",
    "w1 = 1.3\n",
    "b1 = -0.1\n",
    "\n",
    "# Then we define the neuron activation.\n",
    "def a1(a0) :\n",
    "  return σ(w1 * a0 + b1)\n",
    "  \n",
    "# Finally let's try the network out!\n",
    "# Replace x with 0 or 1 below,\n",
    "a1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not very good! But it's not trained yet; experiment by changing the weight and bias and see what happens.\n",
    "\n",
    "Choose the weight and bias that gives the best result for a _NOT function_ out of all the options presented.\n",
    "\n",
    "**$w^{(1)}=-5,b^{(1)}=5$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "Let's extend our simple network to include more neurons.\n",
    "\n",
    "![picture alt](https://i.ibb.co/smYPNtV/JKIDlfih-Eee-APQpr-C3-K2-Bg-66921c651e175da7cc8b805860da4225-simple.png)\n",
    "\n",
    "We now have a slightly changed notation. The neurons which are labeled by their layer with a superscript in brackets, are now also labeled with their number in that layer as a subscript, and form vectors $\\mathbf{a}^{(0)}$ and $\\mathbf{a}^{(1)}$.\n",
    "\n",
    "The weights now form a matrix $\\mathbf{W}^{(1)}$, where each element, $w^{(1)}_{ij}$, is the link between the neuron $j$ in the previous layer and neuron $i$ in the current layer. For example $w^{(1)}_{12}$ is highlighted linking $a^{(0)}_2$ to $a^{(1)}_1$.\n",
    "\n",
    "The biases similarly form a vector $\\mathbf{b}^{(1)}$.\n",
    "\n",
    "We can update our activation function to give,\n",
    "\n",
    "$\\mathbf{a}=\\sigma(\\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)})$.\n",
    "\n",
    "where all the quantities of interest have been _upgraded_ to their vector and matrix form and $\\sigma$ acts upon each element of the resulting weighted sum vector separately.\n",
    "\n",
    "For a network with weights $\\mathbf{W}^{(1)}=\n",
    "\\begin{bmatrix} \n",
    "-2 & 4 & -1 \\\\ \n",
    "6 & 0 & -3 \\\\\n",
    "\\end{bmatrix}$, and bias $\\mathbf{b}=\n",
    "\\begin{bmatrix} \n",
    "0.1 \\\\ \n",
    "-2.5 \\\\\n",
    "\\end{bmatrix}$, calculate the output, $\\mathbf{a}^{(1)}$, given an input vector,\n",
    "\n",
    "$\\mathbf{a}^{(0)}=\n",
    "\\begin{bmatrix} \n",
    "0.3 \\\\ \n",
    "0.4 \\\\\n",
    "0.1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "You may do this calculation either by hand (to 2 decimal places), or by writing python code. Input your answer into the code block below.\n",
    "\n",
    "(If you chose to code, remember that you can use the @ operator in Python to perform operate a matrix on a vector.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set up the network.\n",
    "sigma = np.tanh\n",
    "W = np.array([[-2, 4, -1],[6, 0, -3]])\n",
    "b = np.array([0.1, -2.5])\n",
    "\n",
    "# Define our input vector\n",
    "x = np.array([0.3, 0.4, 0.1])\n",
    "\n",
    "# Calculate the values by hand,\n",
    "# and replace a1_0 and a1_1 here (to 2 decimal places)\n",
    "# (Or if you feel adventurous, find the values with code!)\n",
    "a1 = np.array([a1_0, a1_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a1 = sigma(W @ x + b)**\n",
    "\n",
    "**[ 0.76159416 -0.76159416]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "Now let's look at a network with a _hidden layer_.\n",
    "\n",
    "![picture alt](https://i.ibb.co/bmdjZJD/JKIDlfih-Eee-APQpr-C3-K2-Bg-66921c651e175da7cc8b805860da4225-simple.png)\n",
    "\n",
    "Here, data is input at layer (0), this activates neurons in layer (1), which become the inputs for neurons in layer (2).\n",
    "\n",
    "(We've stopped explicitly drawing the biases here.)\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "**The number of weights in a layer is the product of the input and output neurons to that layer.**\n",
    "\n",
    "**This neural network has 5 biases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "Which of the following statements about the neural network from the previous question are true?\n",
    "\n",
    "**$\\mathbf{a}^{(2)}=\\sigma(\\mathbf{W}^{(2)} \\sigma(\\mathbf{W}^{(1)} \\mathbf{a}^{(0)} + \\mathbf{b}^{(1)}) + \\mathbf{b}^{(2)})$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "So far, we have concentrated mainly on the structure of neural networks, let's look a bit closer at the _function_, and what the parts actually do.\n",
    "\n",
    "We'll introduce another network, this time with a one dimensional input, a one dimensional output, and a hidden layer with two neurons.\n",
    "\n",
    "Use the tool below to change the values of the four weights and three biases, and observe what effect this has on the network's function.\n",
    "\n",
    "With the weights and biases set here, observe how $a^{(1)}_0$ activates when $a^{(0)}_0$ is active, and $a^{(1)}_1$ activates when $a^{(0)}_0$ is inactive. Then the output neuron, $a^{(2)}_0$, activates when neither $a^{(1)}_0$ nor $a^{(1)}_1$ are too active.\n",
    "\n",
    "(Interact with the plugin below to score the point for this question.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
