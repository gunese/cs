{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 3 - Practice Quiz: Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**1.**\n",
    "In this exercise we'll look in more detail about back-propagation, using the chain rule, in order to train our neural networks.\n",
    "\n",
    "Let's look again at our two-node network.\n",
    "\n",
    "![picture alt](https://i.ibb.co/zJ6y4nh/JKIDlfih-Eee-APQpr-C3-K2-Bg-66921c651e175da7cc8b805860da4225-simple.png)\n",
    "\n",
    "Recall the activation equations are,\n",
    "\n",
    "$a(1)= \\sigma(z^{(1)})$\n",
    "\n",
    "$z(1)= w^{(1)} a^{(0)} + b^{(1)}$.\n",
    "\n",
    "Where we've introduced $z^{(1)}$ as the weighted sum of activation and bias.\n",
    "\n",
    "We can formalize how good (or bad) our neural network is at getting the desired behavior. For a particular input, $x$, and desired output $y$, we can define the _cost_ of that specific _training example_ as the square of the difference between the network's output and the desired output, that is,\n",
    "\n",
    "$C_k=(a^{(1)}-y)^2$\n",
    "\n",
    "Where $k$ labels the training example and $a^{(1)}$ is assumed to be the activation of the output neuron when the input neuron $a^{(0)}$ is set to $x$\n",
    "\n",
    "We'll go into detail about how to apply this to an entire set of training data later on. But for now, let's look at our toy example.\n",
    "\n",
    "Recall our _NOT function_ example from the previous quiz. For the input $x = 1$ we would like that the network outputs $y = 0$. For the starting weight and bias $w^{(1)}=1.3$ and $b^{(1)} = -0.1$, the network actually outputs $a^{(1)}=0.834$. If we work out the cost function for this example, we get \n",
    "\n",
    "$C_1 = (0.834 - 0)^2 = 0.696$\n",
    "\n",
    "Do the same calculation for an input $x=0$ and desired output $y=1$. Use the code block to help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we set the state of the network\n",
    "σ = np.tanh\n",
    "w1 = 1.3\n",
    "b1 = -0.1\n",
    "\n",
    "# Then we define the neuron activation.\n",
    "def a1(a0) :\n",
    "  z = w1 * a0 + b1\n",
    "  return σ(z)\n",
    "\n",
    "# Experiment with different values of x below.\n",
    "x = 0\n",
    "a1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is $C_0$ in this particular case? Give your result to 1 decimal place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C0 = (a1(0) - 1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$C_0 = 1.2$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**2.**\n",
    "The cost function of a training set is the average of the individual cost functions of the data in the training set,\n",
    "\n",
    "$C=\\frac{1}{N} \\Sigma_k C_k$\n",
    "\n",
    "where $N$ is the number of examples in the training set.\n",
    "\n",
    "For the NOT function we've been considering, where we have two examples in our training set, $(x=0,y=1)$ and $(x=1,y=0)$, the training set cost function is $C = \\frac{1}{2}(C_0 + C_1)$.\n",
    "\n",
    "Since our parameter space is 2D, $(w^{(1)}$ and $b^{(1)}$, we can draw the total cost function for this neural network as a contour map.\n",
    "\n",
    "![picture alt](https://i.ibb.co/6Wknvf6/n-IIp-YPs-KEeet-Ih-ICv-THYsg-877cd5b466f026f62403e1c26c91ac97-contour.png)\n",
    "\n",
    "Here white represents low costs and black represents high costs.\n",
    "\n",
    "Which of the following statements are true?\n",
    "\n",
    "**The optimal configuration lies somewhere along the line $b=-w$.**\n",
    "\n",
    "**Descending perpendicular to the contours will improve the performance of the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**3.**\n",
    "To improve the performance of the neural network on the training data, we can vary the weight and bias. We can calculate the derivative of the example cost with respect to these quantities using the chain rule.\n",
    "\n",
    "$\\frac{\\partial{C_k}}{\\partial{w^{(1)}}} = \\frac{\\partial{C_k}}{\\partial{a^{(1)}}} \\frac{\\partial{a^{(1)}}}{\\partial{z^{(1)}}} \\frac{\\partial{z^{(1)}}}{\\partial{w^{(1)}}}$\n",
    "\n",
    "$\\frac{\\partial{C_k}}{\\partial{b^{(1)}}} = \\frac{\\partial{C_k}}{\\partial{a^{(1)}}} \\frac{\\partial{a^{(1)}}}{\\partial{z^{(1)}}} \\frac{\\partial{z^{(1)}}}{\\partial{b^{(1)}}}$\n",
    "\n",
    "Individually, these derivatives take fairly simple form. Go ahead and calculate them. We'll repeat the defining equations for convenience,\n",
    "\n",
    "$a(1)= \\sigma(z^{(1)})$\n",
    "\n",
    "$z(1)= w^{(1)} a^{(0)} + b^{(n)}$\n",
    "\n",
    "$C_k=(a^{(1)}-y)^2$\n",
    "\n",
    "Select all true statements below.\n",
    "\n",
    "**$\\frac{\\partial{z^{(1)}}}{\\partial{w^{(1)}}}=a^{(0)}$**\n",
    "\n",
    "**$\\frac{\\partial{z^{(1)}}}{\\partial{b^{(1)}}}=1$**\n",
    "\n",
    "**$\\frac{\\partial{a^{(1)}}}{\\partial{z^{(1)}}}=\\sigma'(z^{(1)})$**\n",
    "\n",
    "**$\\frac{\\partial{C_{k}}}{\\partial{a^{(1)}}}=2({a^{(1)}} - y)$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**4.**\n",
    "Using your answer to the previous question, let's see it implemented in code.\n",
    "\n",
    "The following code block has an example implementation of $\\frac{\\partial{C_{k}}}{\\partial{w^{(1)}}}$. It is up to you to implement $\\frac{\\partial{C_{k}}}{\\partial{b^{(1)}}}$.\n",
    "\n",
    "Don't worry if you don't know exactly how the code works. It's more important that you get a feel for what is going on.\n",
    "\n",
    "We will introduce the following derivative in the code,\n",
    "\n",
    "$\\frac{d}{dz} tanh(z) = \\frac{1}{cosh^2 z}$.\n",
    "\n",
    "Complete the function 'dCdb' below. Replace the ??? towards the bottom, with the expression you calculated in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define our sigma function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Next define the feed-forward equation.\n",
    "def a1 (w1, b1, a0) :\n",
    "  z = w1 * a0 + b1\n",
    "  return sigma(z)\n",
    "\n",
    "# The individual cost function is the square of the difference between\n",
    "# the network output and the training data output.\n",
    "def C (w1, b1, x, y) :\n",
    "  return (a1(w1, b1, x) - y)**2\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the weight.\n",
    "def dCdw (w1, b1, x, y) :\n",
    "  z = w1 * x + b1\n",
    "  dCda = 2 * (a1(w1, b1, x) - y) # Derivative of cost with activation\n",
    "  dadz = 1/np.cosh(z)**2 # derivative of activation with weighted sum z\n",
    "  dzdw = x # derivative of weighted sum z with weight\n",
    "  return dCda * dadz * dzdw # Return the chain rule product.\n",
    "\n",
    "# This function returns the derivative of the cost function with\n",
    "# respect to the bias.\n",
    "# It is very similar to the previous function.\n",
    "# You should complete this function.\n",
    "def dCdb (w1, b1, x, y) :\n",
    "  z = w1 * x + b1\n",
    "  dCda = 2 * (a1(w1, b1, x) - y)\n",
    "  dadz = 1/np.cosh(z)**2\n",
    "  \"\"\" Change the next line to give the derivative of\n",
    "      the weighted sum, z, with respect to the bias, b. \"\"\"\n",
    "  dzdb = ???\n",
    "  return dCda * dadz * dzdb\n",
    "\n",
    "\"\"\"Test your code before submission:\"\"\"\n",
    "# Let's start with an unfit weight and bias.\n",
    "w1 = 2.3\n",
    "b1 = -1.2\n",
    "# We can test on a single data point pair of x and y.\n",
    "x = 0\n",
    "y = 1\n",
    "# Output how the cost would change\n",
    "# in proportion to a small change in the bias\n",
    "print( dCdb(w1, b1, x, y) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dzdb = 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**5.**\n",
    "Recall that when we add more neurons to the network, our quantities are upgraded to vectors or matrices.\n",
    "\n",
    "![picture alt](https://i.ibb.co/VQ7sbkR/n-IIp-YPs-KEeet-Ih-ICv-THYsg-877cd5b466f026f62403e1c26c91ac97-contour.png)\n",
    "\n",
    "$\\mathbf{a^{(1)}}= \\sigma(\\mathbf{z^{(1)}})$\n",
    "\n",
    "$\\mathbf{z^{(1)}} = \\mathbf{W^{(1)}} \\mathbf{a^{(0)}} + \\mathbf{b^{(1)}}$.\n",
    "\n",
    "The individual cost functions remain scalars. Instead of becoming vectors, the components are summed over each output neuron.\n",
    "\n",
    "$C=\\Sigma_i (a^{(1)}_i - y^i)^2$\n",
    "\n",
    "Note here that $i$ labels the output neuron and is summed over, whereas $k$ labels the training example.\n",
    "\n",
    "The training data becomes a vector too,\n",
    "\n",
    "$x \\rightarrow \\mathbf{x}$ and has the same number of elements as input neurons.\n",
    "\n",
    "$y \\rightarrow \\mathbf{y}$ and has the same number of elements as output neurons.\n",
    "\n",
    "This allows us to write the cost function in vector form using the modulus squared,\n",
    "\n",
    "$C_k = |a^{(1)} - y|^2$\n",
    "\n",
    "Use the code block below to play with calculating the cost function for this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the activation function.\n",
    "sigma = np.tanh\n",
    "\n",
    "# Let's use a random initial weight and bias.\n",
    "W = np.array([[-0.94529712, -0.2667356 , -0.91219181],\n",
    "              [ 2.05529992,  1.21797092,  0.22914497]])\n",
    "b = np.array([ 0.61273249,  1.6422662 ])\n",
    "\n",
    "# define our feed forward function\n",
    "def a1 (a0) :\n",
    "  # Notice the next line is almost the same as previously,\n",
    "  # except we are using matrix multiplication rather than scalar multiplication\n",
    "  # hence the '@' operator, and not the '*' operator.\n",
    "  z = W @ a0 + b\n",
    "  # Everything else is the same though,\n",
    "  return sigma(z)\n",
    "\n",
    "# Next, if a training example is,\n",
    "x = np.array([0.1, 0.5, 0.6])\n",
    "y = np.array([0.25, 0.75])\n",
    "\n",
    "# Then the cost function is,\n",
    "d = a1(x) - y # Vector difference between observed and expected activation\n",
    "C = d @ d # Absolute value squared of the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the initial weights and biases, what is the example cost function, $C_k$, when, $x =\n",
    "\\begin{bmatrix} \n",
    "0.7 \\\\ \n",
    "0.6 \\\\ \n",
    "0.2\n",
    "\\end{bmatrix}$ and $y =\n",
    "\\begin{bmatrix} \n",
    "0.9 \\\\ \n",
    "0.6\n",
    "\\end{bmatrix}$?\n",
    "\n",
    "Give your answer to 1 decimal place.\n",
    "\n",
    "**1.8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <br/>**6.**\n",
    "Let's now consider a neural network with hidden layers.\n",
    "\n",
    "![picture alt](https://i.ibb.co/bmdjZJD/JKIDlfih-Eee-APQpr-C3-K2-Bg-66921c651e175da7cc8b805860da4225-simple.png)\n",
    "\n",
    "Training this network is done by _back-propagation_ because we start at the output layer and calculate derivatives backwards towards the input layer with the chain rule.\n",
    "\n",
    "Let's see how this works.\n",
    "\n",
    "If we wanted to calculate the derivative of the cost with respect to the weight or bias of the final layer, then this is the same as previously (but now in vector form):\n",
    "\n",
    "$\\frac{\\partial{C_k}}{\\partial{\\mathbf{W^{(2)}}}} = \\frac{\\partial{C_k}}{\\partial{\\mathbf{a^{(2)}}}} \\frac{\\partial{\\mathbf{a^{(2)}}}}{\\partial{\\mathbf{z^{(2)}}}} \\frac{\\partial{\\mathbf{z^{(2)}}}}{\\partial{\\mathbf{W^{(2)}}}}$\n",
    "\n",
    "With a similar term for the bias. If we want to calculate the derivative with respects to weights of the previous layer, we use the expression,\n",
    "\n",
    "$\\frac{\\partial{C_k}}{\\partial{\\mathbf{W^{(1)}}}} = \\frac{\\partial{C_k}}{\\partial{\\mathbf{a^{(2)}}}} \\frac{\\partial{\\mathbf{a^{(2)}}}}{\\partial{\\mathbf{a^{(1)}}}} \\frac{\\partial{\\mathbf{a^{(1)}}}}{\\partial{\\mathbf{z^{(1)}}}} \\frac{\\partial{\\mathbf{z^{(1)}}}}{\\partial{\\mathbf{W^{(1)}}}}$\n",
    "\n",
    "Where $\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}$ itself can be expanded to,\n",
    "\n",
    "$\\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}} = \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}} \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{a}^{(1)}}$\n",
    "\n",
    "This can be generalized to any layer,\n",
    "\n",
    "$\\frac{\\partial{C_k}}{\\partial{\\mathbf{W^{(i)}}}} = \\frac{\\partial{C_k}}{\\partial{\\mathbf{a^{(N)}}}} \\frac{\\partial{\\mathbf{a^{(N)}}}}{\\partial{\\mathbf{a^{(N-1)}}}} \\frac{\\partial{\\mathbf{a^{(N-1)}}}}{\\partial{\\mathbf{a^{(N-2)}}}} ... \\frac{\\partial{\\mathbf{a^{(i+1)}}}}{\\partial{\\mathbf{a^{(i)}}}} \\frac{\\partial{\\mathbf{a^{(i)}}}}{\\partial{\\mathbf{z^{(i)}}}} \\frac{\\partial{\\mathbf{z^{(1)}}}}{\\partial{\\mathbf{W^{(1)}}}}$\n",
    "\n",
    "By further application of the chain rule.\n",
    "\n",
    "Choose the correct expression for the derivative,\n",
    "\n",
    "$\\frac{\\partial{\\mathbf{a^{(j)}}}}{\\partial{\\mathbf{a^{(j-1)}}}}$\n",
    "\n",
    "Remembering the activation equations are,\n",
    "\n",
    "$a^{(n)}=\\sigma(z^{(n)})$\n",
    "\n",
    "$z^{(n)} = w^{(n)} a^{(n-1)} + b^{(n)}$.\n",
    "\n",
    "**$\\sigma'(\\mathbf{z}^{(j)}) \\mathbf{W}^{(j)}$**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
